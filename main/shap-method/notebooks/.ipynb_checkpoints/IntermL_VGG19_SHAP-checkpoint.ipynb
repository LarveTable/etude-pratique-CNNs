{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain an Intermediate Layer of VGG16 on ImageNet (PyTorch)\n",
    "\n",
    "Explaining a prediction in terms of the original input image is harder than explaining the predicition in terms of a higher convolutional layer (because the higher convolutional layer is closer to the output). This notebook gives a simple example of how to use GradientExplainer to do explain a model output with respect to the 7th layer of the pretrained VGG16 network.\n",
    "\n",
    "Note that by default 200 samples are taken to compute the expectation. To run faster you can lower the number of samples per explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 16:14:25.731675: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-12 16:14:26.049364: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-12 16:14:26.049443: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-12 16:14:26.051350: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-12 16:14:26.225032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-12 16:14:27.335302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "from keras.preprocessing import image\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    if image.max() > 1:\n",
    "        image /= 255\n",
    "    image = (image - mean) / std\n",
    "    # in addition, roll the axis so that they suit pytorch\n",
    "    return torch.tensor(image.swapaxes(-1, 1).swapaxes(2, 3)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: (1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "X, y = shap.datasets.imagenet50()\n",
    "\n",
    "img_path = \"../data/Trousse.jpg\"\n",
    "  \n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "im = image.img_to_array(img)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "print('Input image shape:', im.shape)\n",
    "X[0] = im\n",
    "\n",
    "X /= 255\n",
    "\n",
    "to_explain = X[[0,4]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = shap.GradientExplainer((model, model.features[7]), normalize(X))\n",
    "shap_values, indexes = e.shap_values(\n",
    "    normalize(to_explain), ranked_outputs=3, nsamples=20\n",
    ")\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap_values = [np.swapaxes(np.swapaxes(s, 2, 3), 1, -1) for s in shap_values]\n",
    "\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitions \n",
    "\n",
    "# python function to get model output; replace this function with your own model function.\n",
    "def f(x):\n",
    "    tmp = x.copy()\n",
    "    #preprocess_input(tmp)\n",
    "    return model(tmp)\n",
    "\n",
    "# define a masker that is used to mask out partitions of the input image.\n",
    "masker = shap.maskers.Image(\"inpaint_telea\", X[0].shape)\n",
    "\n",
    "# create an explainer with model and image masker\n",
    "explainer = shap.Explainer(f, masker, output_names=class_names)\n",
    "\n",
    "# here we explain two images using 500 evaluations of the underlying model to estimate the SHAP values\n",
    "shap_values = explainer(\n",
    "    X[1:3], max_evals=100, batch_size=50, outputs=shap.Explanation.argsort.flip[:4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_values_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain with local smoothing\n",
    "\n",
    "Gradient explainer uses expected gradients, which merges ideas from integrated gradients, SHAP, and SmoothGrad into a single expection equation. To use smoothing like SmoothGrad just set the local_smoothing parameter to something non-zero. This will add normally distributed noise with that standard deviation to the input during the expectation calculation. It can create smoother feature attributions that better capture correlated regions of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that because the inputs are scaled to be between 0 and 1, the local smoothing also has to be\n",
    "# scaled compared to the Keras model\n",
    "explainer = shap.GradientExplainer(\n",
    "    (model, model.features[7]), normalize(X), local_smoothing=0.5\n",
    ")\n",
    "shap_values, indexes = explainer.shap_values(\n",
    "    normalize(to_explain), ranked_outputs=2, nsamples=200\n",
    ")\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap_values = [np.swapaxes(np.swapaxes(s, 2, 3), 1, -1) for s in shap_values]\n",
    "\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
